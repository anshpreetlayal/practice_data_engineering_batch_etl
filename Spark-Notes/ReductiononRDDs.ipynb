{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Operations on RDDs: `fold` and `aggregate`\n",
    "\n",
    "In Apache Spark, `fold` and `aggregate` are two reduction operations that allow you to combine the elements of an RDD into a single result. While both operations are used for aggregation, they differ in their usage and capabilities.\n",
    "\n",
    "### `fold` Operation\n",
    "\n",
    "- **Definition**: The `fold` operation in Spark is used to aggregate the elements of an RDD using a binary function and an initial \"zero value.\" The zero value is used as an initial accumulator for each partition, and then the binary function is used to combine the accumulator with each element of the partition.\n",
    "\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  def fold(zeroValue: T)(op: (T, T) => T): T\n",
    "  ```\n",
    "\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "  val sum = rdd.fold(0)((acc, ele) => acc + ele)\n",
    "  ```\n",
    "\n",
    "- **Behavior**:\n",
    "  - The `fold` operation starts with the `zeroValue` for each partition.\n",
    "  - It then applies the `op` function to combine the `zeroValue` with each element of the partition to produce a single result for that partition.\n",
    "  - Finally, it combines the results from all partitions using the `op` function to produce the final result.\n",
    "\n",
    "### `aggregate` Operation\n",
    "\n",
    "- **Definition**: The `aggregate` operation in Spark is similar to `fold` but allows you to return a different type of result. It takes three arguments: an initial \"zero value,\" a function to combine elements within each partition, and a function to combine results from different partitions.\n",
    "\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  def aggregate[U](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U\n",
    "  ```\n",
    "\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "  val result = rdd.aggregate((0, 0))(\n",
    "    (acc, ele) => (acc._1 + ele, acc._2 + 1),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  val avg = result._1.toDouble / result._2\n",
    "  ```\n",
    "\n",
    "- **Behavior**:\n",
    "  - The `aggregate` operation starts with the `zeroValue` for each partition.\n",
    "  - It then applies the `seqOp` function to combine each element of the partition with the `zeroValue` to produce a partial result for that partition.\n",
    "  - Finally, it combines the partial results from all partitions using the `combOp` function to produce the final result.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "- **Use Case**:\n",
    "  - Use `fold` when you need to aggregate elements of an RDD into a single result using a simple binary function.\n",
    "  - Use `aggregate` when you need more control over the aggregation process, such as when you need to return a different type of result or when you need to perform different aggregation operations within and across partitions.\n",
    "\n",
    "- **Performance**:\n",
    "  - `fold` can be more efficient than `aggregate` for simple aggregation tasks because it avoids the overhead of creating and merging partial results.\n",
    "  - `aggregate` is more flexible but may incur higher overhead due to the need to create and merge partial results.\n",
    "\n",
    "\n",
    "\n",
    "### **Distributed Key-Value Pairs in Spark RDDs**\n",
    "\n",
    "In Apache Spark, RDDs (Resilient Distributed Datasets) can represent key-value pairs, where each element in the RDD is a tuple `(key, value)`. This allows you to perform operations that are specific to key-value pairs, such as grouping by key, joining, and aggregating.\n",
    "\n",
    "### Creating a Pair RDD\n",
    "\n",
    "You can create a Pair RDD from an existing RDD by using the `map` transformation to convert each element into a key-value pair. For example:\n",
    "```scala\n",
    "val rdd = sc.parallelize(Seq(\"key1\" -> 1, \"key2\" -> 2, \"key1\" -> 3))\n",
    "val pairRDD = rdd.map { case (key, value) => (key, value) }\n",
    "```\n",
    "\n",
    "### Transformation Operations on Pair RDDs\n",
    "\n",
    "1. **Grouping by Key**: Use the `groupByKey` transformation to group values with the same key together.\n",
    "   ```scala\n",
    "   val groupedRDD = pairRDD.groupByKey()\n",
    "   ```\n",
    "\n",
    "2. **Reduce by Key**: Use the `reduceByKey` transformation to apply a reduction function to values with the same key.\n",
    "   ```scala\n",
    "   val sumByKeyRDD = pairRDD.reduceByKey(_ + _)\n",
    "   ```\n",
    "\n",
    "3. **Sorting by Key**: Use the `sortByKey` transformation to sort the RDD by key.\n",
    "   ```scala\n",
    "   val sortedRDD = pairRDD.sortByKey()\n",
    "   ```\n",
    "\n",
    "\n",
    "4. **Joining**: Use the `join` transformation to join two Pair RDDs based on their keys.\n",
    "   ```scala\n",
    "   val otherRDD: RDD[(String, Int)] = ...\n",
    "   val joinedRDD = pairRDD.join(otherRDD)\n",
    "   ```\n",
    "\n",
    "5. **Aggregating by Key**: Use the `aggregateByKey` transformation to perform aggregation on values with the same key.\n",
    "   ```scala\n",
    "   val avgByKeyRDD = pairRDD.aggregateByKey((0, 0))(\n",
    "     (acc, value) => (acc._1 + value, acc._2 + 1),\n",
    "     (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "   ).mapValues { case (sum, count) => sum.toDouble / count }\n",
    "   ```\n",
    "\n",
    "### Actions on Pair RDDs\n",
    "\n",
    "1. **Collect**: Use the `collect` action to collect all the elements of the Pair RDD as an array.\n",
    "   ```scala\n",
    "   val collectedArray = pairRDD.collect()\n",
    "   ```\n",
    "\n",
    "2. **Count**: Use the `count` action to count the number of elements in the Pair RDD.\n",
    "   ```scala\n",
    "   val count = pairRDD.count()\n",
    "   ```\n",
    "\n",
    "3. **Take**: Use the `take` action to take the first `n` elements of the Pair RDD.\n",
    "   ```scala\n",
    "   val firstElements = pairRDD.take(5)\n",
    "   ```\n",
    "\n",
    "4. **ForEach**: Use the `foreach` action to apply a function to each element of the Pair RDD.\n",
    "   ```scala\n",
    "   pairRDD.foreach { case (key, value) => println(s\"Key: $key, Value: $value\") }\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "### Transformation Operations on Pair RDDs\n",
    "Pair RDDs in Apache Spark provide a rich set of operations that allow you to perform transformations and actions specific to key-value pairs. Here are some common operations you can perform on Pair RDDs:\n",
    "\n",
    "1. **`groupByKey()`**: Groups the values for each key in the RDD into a single sequence.\n",
    "   ```scala\n",
    "   val groupedRDD = pairRDD.groupByKey()\n",
    "   ```\n",
    "\n",
    "2. **`reduceByKey(func)`**: Reduces the values for each key using the specified commutative and associative function `func`.\n",
    "   ```scala\n",
    "   val sumRDD = pairRDD.reduceByKey(_ + _)\n",
    "   ```\n",
    "\n",
    "3. **`mapValues(func)`**: Applies a function `func` to each value of the Pair RDD without changing the keys.\n",
    "   ```scala\n",
    "   val mappedRDD = pairRDD.mapValues(value => value * 2)\n",
    "   ```\n",
    "\n",
    "4. **`flatMapValues(func)`**: Similar to `mapValues`, but the function `func` can return multiple values, which are then flattened into the output.\n",
    "   ```scala\n",
    "   val flatMappedRDD = pairRDD.flatMapValues(value => Seq(value, value * 2))\n",
    "   ```\n",
    "\n",
    "5. **`keys()`**: Returns an RDD containing only the keys of the Pair RDD.\n",
    "   ```scala\n",
    "   val keysRDD = pairRDD.keys\n",
    "   ```\n",
    "\n",
    "6. **`values()`**: Returns an RDD containing only the values of the Pair RDD.\n",
    "   ```scala\n",
    "   val valuesRDD = pairRDD.values\n",
    "   ```\n",
    "\n",
    "\n",
    "7. **`sortByKey(ascending: Boolean = true)`**: Sorts the Pair RDD by key, with an optional parameter to specify ascending or descending order.\n",
    "   ```scala\n",
    "   val sortedRDD = pairRDD.sortByKey(ascending = false)\n",
    "   ```\n",
    "\n",
    "8. **`cogroup(otherRDD)`**: Groups the values of this Pair RDD and another RDD sharing the same key.\n",
    "   ```scala\n",
    "   val cogroupedRDD = pairRDD.cogroup(otherRDD)\n",
    "   ```\n",
    "\n",
    "### Action Operations on Pair RDDs\n",
    "\n",
    "1. **`collectAsMap()`**: Collects the result as a Map from keys to values.\n",
    "   ```scala\n",
    "   val resultMap = pairRDD.collectAsMap()\n",
    "   ```\n",
    "\n",
    "2. **`countByKey()`**: Counts the number of elements for each key and returns the result as a Map.\n",
    "   ```scala\n",
    "   val countMap = pairRDD.countByKey()\n",
    "   ```\n",
    "\n",
    "3. **`lookup(key)`**: Returns all values associated with the given key.\n",
    "   ```scala\n",
    "   val valuesForKey = pairRDD.lookup(\"key\")\n",
    "   ```\n",
    "\n",
    "4. **`foreach(func)`**: Applies a function `func` to each key-value pair in the Pair RDD.\n",
    "   ```scala\n",
    "   pairRDD.foreach { case (key, value) => println(s\"Key: $key, Value: $value\") }\n",
    "   ```\n",
    "\n",
    "5. **`reduceByKeyLocally(func)`**: Reduces the values for each key in the Pair RDD locally on each partition.\n",
    "   ```scala\n",
    "   val localResultMap = pairRDD.reduceByKeyLocally(_ + _)\n",
    "   ```\n",
    "\n",
    "### **Joins in Apache Spark**\n",
    "\n",
    "In Apache Spark, joins are operations that combine two RDDs (or DataFrames) based on a common key. Spark supports several types of joins, including inner join, outer join, left outer join, and right outer join.\n",
    "\n",
    "### Inner Join\n",
    "\n",
    "- **Definition**: Inner join returns only the rows where there is a match in both RDDs based on the join key.\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  val joinedRDD = rdd1.join(rdd2)\n",
    "  ```\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val rdd1 = sc.parallelize(Seq(\"A\" -> 1, \"B\" -> 2, \"C\" -> 3))\n",
    "  val rdd2 = sc.parallelize(Seq(\"A\" -> \"apple\", \"B\" -> \"banana\", \"D\" -> \"date\"))\n",
    "  val innerJoinedRDD = rdd1.join(rdd2)\n",
    "  ```\n",
    "- **Result**:\n",
    "  ```\n",
    "  (A, (1, apple))\n",
    "  (B, (2, banana))\n",
    "  ```\n",
    "\n",
    "### Outer Join\n",
    "\n",
    "- **Definition**: Outer join returns all rows from both RDDs, with null values for missing keys in either RDD.\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  val outerJoinedRDD = rdd1.fullOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val outerJoinedRDD = rdd1.fullOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Result**:\n",
    "  ```\n",
    "  (A, (Some(1), Some(apple)))\n",
    "  (B, (Some(2), Some(banana)))\n",
    "  (C, (Some(3), None))\n",
    "  (D, (None, Some(date)))\n",
    "  ```\n",
    "\n",
    "### Left Outer Join\n",
    "\n",
    "- **Definition**: Left outer join returns all rows from the left RDD, and the matched rows from the right RDD, with null values for missing keys in the right RDD.\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  val leftOuterJoinedRDD = rdd1.leftOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val leftOuterJoinedRDD = rdd1.leftOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Result**:\n",
    "  ```\n",
    "  (A, (1, Some(apple)))\n",
    "  (B, (2, Some(banana)))\n",
    "  (C, (3, None))\n",
    "  ```\n",
    "\n",
    "### Right Outer Join\n",
    "\n",
    "- **Definition**: Right outer join returns all rows from the right RDD, and the matched rows from the left RDD, with null values for missing keys in the left RDD.\n",
    "- **Syntax**:\n",
    "  ```scala\n",
    "  val rightOuterJoinedRDD = rdd1.rightOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Example**:\n",
    "  ```scala\n",
    "  val rightOuterJoinedRDD = rdd1.rightOuterJoin(rdd2)\n",
    "  ```\n",
    "- **Result**:\n",
    "  ```\n",
    "  (A, (Some(1), apple))\n",
    "  (B, (Some(2), banana))\n",
    "  (D, (None, date))\n",
    "  ```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
