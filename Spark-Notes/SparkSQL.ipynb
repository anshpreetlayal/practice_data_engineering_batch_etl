{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "Spark SQL is a component of Apache Spark that introduces a structured data processing framework along with a SQL interface. It allows you to query structured data using SQL syntax as well as programmatically using the DataFrame API, which provides a more type-safe and efficient way to work with structured data compared to traditional RDDs.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **DataFrame**: Spark SQL introduces the DataFrame abstraction, which represents a distributed collection of data organized into named columns. DataFrames can be created from a variety of sources, including structured data files (e.g., CSV, JSON), Hive tables, and existing RDDs.\n",
    "\n",
    "2. **SQL Queries**: Spark SQL allows you to write SQL queries to manipulate and analyze DataFrames. These queries are translated into optimized Spark jobs by the Catalyst optimizer.\n",
    "\n",
    "3. **Catalyst Optimizer**: Catalyst is Spark's query optimization framework, which applies various optimization rules and strategies to optimize SQL queries and DataFrames. It helps in improving the performance of Spark SQL queries by optimizing the logical and physical execution plans.\n",
    "\n",
    "4. **Datasets**: Datasets are a newer API introduced in Spark that combines the benefits of RDDs and DataFrames. Datasets provide type safety and object-oriented programming features while maintaining the performance benefits of DataFrames.\n",
    "\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Ease of Use**: Spark SQL provides a familiar SQL interface for querying structured data, making it easier for users familiar with SQL to work with Spark.\n",
    "\n",
    "2. **Performance**: The Catalyst optimizer optimizes SQL queries for better performance, leading to faster execution times compared to traditional RDD-based operations.\n",
    "\n",
    "3. **Integration**: Spark SQL integrates seamlessly with other Spark components, such as MLlib (machine learning library) and GraphX (graph processing library), allowing you to build end-to-end data pipelines.\n",
    "\n",
    "4. **Compatibility**: Spark SQL is compatible with Hive, enabling you to run existing Hive queries and access Hive metastore tables.\n",
    "\n",
    "\n",
    "\n",
    "### Structured and Unstructured Data in Apache Spark\n",
    "\n",
    "Apache Spark can handle both structured and unstructured data, offering different APIs and libraries tailored to each type.\n",
    "\n",
    "### Structured Data\n",
    "\n",
    "1. **Definition**: Structured data is data that is organized in a specific format, such as tables with rows and columns. Examples include CSV, JSON, Parquet, and relational databases.\n",
    "\n",
    "2. **Handling in Spark**:\n",
    "   - Spark SQL: Spark provides a SQL interface for working with structured data, allowing you to run SQL queries against datasets.\n",
    "   - DataFrames: Spark DataFrames are distributed collections of data organized into named columns, similar to tables in a relational database.\n",
    "\n",
    "3. **Example**:\n",
    "   ```scala\n",
    "   val df = spark.read.format(\"csv\").load(\"data.csv\")\n",
    "   df.show()\n",
    "   ```\n",
    "\n",
    "4. **Benefits**:\n",
    "   - Schema: Structured data often comes with a predefined schema, making it easier to query and analyze.\n",
    "   - Optimization: Spark can optimize operations on structured data, leading to better performance.\n",
    "\n",
    "### Unstructured Data\n",
    "\n",
    "1. **Definition**: Unstructured data is data that does not have a predefined format or structure. Examples include text documents, images, and videos.\n",
    "\n",
    "2. **Handling in Spark**:\n",
    "   - RDDs: Spark RDDs (Resilient Distributed Datasets) can be used to handle unstructured data. RDDs allow you to perform low-level transformations and actions on data.\n",
    "   - MLlib: Spark's machine learning library, MLlib, provides algorithms for processing unstructured data, such as text analysis and image processing.\n",
    "\n",
    "3. **Example**:\n",
    "   ```scala\n",
    "   val rdd = sc.textFile(\"data.txt\")\n",
    "   val words = rdd.flatMap(_.split(\" \"))\n",
    "   ```\n",
    "\n",
    "4. **Challenges**:\n",
    "   - Lack of Schema: Unstructured data often lacks a schema, making it challenging to query and analyze.\n",
    "   - Performance: Processing unstructured data can be more computationally intensive compared to structured data.\n",
    "\n",
    "### Handling Both Types\n",
    "\n",
    "1. **Hybrid Approach**: Spark allows you to work with both structured and unstructured data within the same application. For example, you can use DataFrames for structured data and RDDs for unstructured data in the same Spark job.\n",
    "\n",
    "2. **Example**:\n",
    "   ```scala\n",
    "   val df = spark.read.format(\"csv\").load(\"data.csv\")\n",
    "   val rdd = sc.textFile(\"data.txt\")\n",
    "   ```\n",
    "\n",
    "3. **Benefits**:\n",
    "   - Flexibility: Spark's ability to handle both types of data allows you to build complex data processing pipelines that can accommodate a variety of data formats.\n",
    "\n",
    "\n",
    "### SQL Literal Syntax\n",
    "SQL literal syntax refers to the way SQL queries are written and interpreted by Apache Spark's Catalyst optimizer and Tungsten execution engine. Catalyst is Spark's query optimization framework, and Tungsten is the underlying execution engine that improves Spark's performance by optimizing memory and CPU usage.\n",
    "\n",
    "### Catalyst Optimizer\n",
    "\n",
    "1. **Query Parsing**: Catalyst parses the SQL query and converts it into an abstract syntax tree (AST), which represents the logical plan of the query.\n",
    "\n",
    "2. **Logical Plan Optimization**: Catalyst performs various optimizations on the logical plan, such as predicate pushdown, projection pruning, and constant folding, to optimize the query's execution.\n",
    "\n",
    "3. **Physical Plan Generation**: Catalyst generates a physical plan from the optimized logical plan. The physical plan specifies how the query will be executed, including the data access methods and join strategies.\n",
    "\n",
    "### Tungsten Execution Engine\n",
    "\n",
    "1. **Code Generation**: Tungsten uses code generation techniques to compile parts of the query into Java bytecode, which can be executed efficiently by the JVM.\n",
    "\n",
    "2. **Memory Management**: Tungsten manages memory efficiently by using off-heap memory and optimizing data structures for cache-aware computation.\n",
    "\n",
    "3. **Binary Processing**: Tungsten processes data in binary format, which reduces the overhead of object serialization and deserialization.\n",
    "\n",
    "### SQL Literal Syntax\n",
    "\n",
    "1. **Standard SQL Syntax**: Catalyst and Tungsten support standard SQL syntax for writing queries, including SELECT, FROM, WHERE, GROUP BY, ORDER BY, and JOIN clauses.\n",
    "\n",
    "2. **Expression Language**: Spark's SQL syntax includes an expression language for specifying column transformations, filtering, and aggregations. For example:\n",
    "   - `SELECT col1, col2 FROM table WHERE col1 > 10`\n",
    "   - `SELECT AVG(col1) FROM table GROUP BY col2`\n",
    "\n",
    "3. **Extension Functions**: Spark's SQL syntax also supports extension functions and UDFs (User Defined Functions) for custom data processing logic. For example:\n",
    "   - `SELECT my_udf(col1) FROM table`\n",
    "\n",
    "4. **DataFrame API**: While not SQL syntax per se, Spark's DataFrame API provides a programmatic way to write SQL-like queries in Scala, Java, Python, and R.\n",
    "\n",
    "### DataFrames in Apache Spark\n",
    "\n",
    "DataFrames are a key abstraction in Apache Spark for working with structured data. They provide a higher-level API than RDDs, allowing for easier manipulation and analysis of data. DataFrames represent distributed collections of data organized into named columns, similar to a table in a relational database or a data frame in R or Python pandas.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Schema**: DataFrames have a schema, which defines the structure of the data including column names and data types. This schema allows Spark to optimize query execution and provides a way to enforce data integrity.\n",
    "\n",
    "2. **Lazy Evaluation**: Like RDDs, DataFrames in Spark use lazy evaluation. This means that transformations on DataFrames are not computed immediately but are instead built up as a logical plan. Actions trigger the execution of the logical plan.\n",
    "\n",
    "3. **Immutability**: DataFrames are immutable, meaning that once created, they cannot be changed. Instead, transformations on DataFrames create new DataFrames.\n",
    "\n",
    "4. **Optimization**: Spark's Catalyst optimizer optimizes the execution of DataFrame operations, including predicate pushdown, projection pruning, and constant folding, to improve performance.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```scala\n",
    "// Create a DataFrame from a JSON file\n",
    "val df = spark.read.json(\"path/to/data.json\")\n",
    "\n",
    "// Show the first few rows of the DataFrame\n",
    "df.show()\n",
    "\n",
    "// Select specific columns from the DataFrame\n",
    "val selectedDF = df.select(\"name\", \"age\")\n",
    "\n",
    "// Filter the DataFrame based on a condition\n",
    "val filteredDF = df.filter($\"age\" > 18)\n",
    "\n",
    "// Perform aggregation on the DataFrame\n",
    "val aggDF = df.groupBy(\"gender\").agg(avg(\"age\"))\n",
    "\n",
    "// Write the DataFrame to a parquet file\n",
    "aggDF.write.parquet(\"output/path\")\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Ease of Use**: DataFrames provide a higher-level API compared to RDDs, making it easier to express data transformations and analysis operations.\n",
    "\n",
    "2. **Optimization**: DataFrames leverage Spark's Catalyst optimizer to optimize query execution, leading to better performance.\n",
    "\n",
    "3. **Compatibility**: DataFrames are compatible with various data formats, including JSON, Parquet, and CSV, making it easy to read and write data in different formats.\n",
    "\n",
    "4. **Interoperability**: DataFrames can be easily converted to and from RDDs, allowing you to leverage the flexibility of RDDs when needed.\n",
    "\n",
    "\n",
    "\n",
    "### DataFrame Data Types in Apache Spark\n",
    "\n",
    "Apache Spark's DataFrame API provides a wide range of data types to represent various types of data. These data types are used to define the schema of a DataFrame, specifying the structure of the data including column names and data types. Here are some common DataFrame data types in Spark:\n",
    "\n",
    "1. **StringType**: Represents a string of characters.\n",
    "\n",
    "2. **IntegerType**: Represents a 32-bit signed integer.\n",
    "\n",
    "3. **LongType**: Represents a 64-bit signed integer.\n",
    "\n",
    "4. **DoubleType**: Represents a 64-bit double-precision floating-point number.\n",
    "\n",
    "5. **FloatType**: Represents a 32-bit single-precision floating-point number.\n",
    "\n",
    "6. **DecimalType**: Represents a fixed-precision decimal number with configurable precision and scale.\n",
    "\n",
    "7. **BooleanType**: Represents a boolean value (`true` or `false`).\n",
    "\n",
    "8. **TimestampType**: Represents a timestamp with optional timezone information.\n",
    "\n",
    "9. **DateType**: Represents a date without time information.\n",
    "\n",
    "10. **ArrayType**: Represents an array of elements with a specified data type.\n",
    "\n",
    "11. **MapType**: Represents a map of key-value pairs, where keys and values have specified data types.\n",
    "\n",
    "12. **StructType**: Represents a struct, or a collection of named columns with specified data types.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Define a schema for a DataFrame\n",
    "val schema = StructType(\n",
    "  List(\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = true),\n",
    "    StructField(\"salary\", DoubleType, nullable = true)\n",
    "  )\n",
    ")\n",
    "\n",
    "// Create a DataFrame using the defined schema\n",
    "val df = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(Seq(\n",
    "    Row(\"Alice\", 30, 10000.0),\n",
    "    Row(\"Bob\", 25, 12000.0)\n",
    "  )),\n",
    "  schema\n",
    ")\n",
    "\n",
    "// Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "// we define a schema for a DataFrame with three columns: \"name\" (StringType), \"age\" (IntegerType), and \"salary\" (DoubleType). We then create a DataFrame using this schema and show its contents.\n",
    "\n",
    "```\n",
    "\n",
    "### Transformations on DataFrames in Apache Spark\n",
    "\n",
    "Transformations in Apache Spark's DataFrame API allow you to manipulate the contents of a DataFrame, such as filtering rows, selecting columns, and aggregating data. These transformations are lazily evaluated, meaning that they are not executed until an action is triggered. Here are some common transformations on DataFrames:\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Filter rows based on a condition:\n",
    "\n",
    "```scala\n",
    "val filteredDF = df.filter($\"age\" > 18)\n",
    "```\n",
    "\n",
    "### Selecting Columns\n",
    "\n",
    "Select specific columns from a DataFrame:\n",
    "\n",
    "```scala\n",
    "val selectedDF = df.select(\"name\", \"age\")\n",
    "```\n",
    "\n",
    "### Adding Columns\n",
    "\n",
    "Add a new column to a DataFrame:\n",
    "\n",
    "```scala\n",
    "val newDF = df.withColumn(\"is_adult\", $\"age\" > 18)\n",
    "```\n",
    "\n",
    "### Grouping and Aggregating\n",
    "\n",
    "Group rows by a column and perform aggregation:\n",
    "\n",
    "```scala\n",
    "val aggDF = df.groupBy(\"gender\").agg(avg(\"age\"), max(\"salary\"))\n",
    "```\n",
    "\n",
    "### Joining DataFrames\n",
    "\n",
    "Join two DataFrames based on a common column:\n",
    "\n",
    "```scala\n",
    "val joinedDF = df1.join(df2, df1(\"id\") === df2(\"id\"))\n",
    "```\n",
    "\n",
    "### Sorting\n",
    "\n",
    "Sort rows based on one or more columns:\n",
    "\n",
    "```scala\n",
    "val sortedDF = df.sort($\"age\".desc)\n",
    "```\n",
    "\n",
    "### Dropping Columns\n",
    "\n",
    "Drop one or more columns from a DataFrame:\n",
    "\n",
    "```scala\n",
    "val droppedDF = df.drop(\"salary\")\n",
    "```\n",
    "\n",
    "### Adding Rows\n",
    "\n",
    "Union two DataFrames to add rows:\n",
    "\n",
    "```scala\n",
    "val combinedDF = df1.union(df2)\n",
    "```\n",
    "\n",
    "### Handling Null Values\n",
    "\n",
    "Fill null values in a column with a specified value:\n",
    "\n",
    "```scala\n",
    "val filledDF = df.na.fill(0, Seq(\"age\"))\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
