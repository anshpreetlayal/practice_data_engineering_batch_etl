{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformation on Two RDDs**\n",
    "Apache Spark allows you to perform various transformations on two RDDs, such as union, intersection, subtract, and cartesian. These transformations enable you to combine or manipulate two RDDs in different ways. Here's a detailed explanation of each:\n",
    "\n",
    "### Union\n",
    "\n",
    "The `union` transformation combines two RDDs into a single RDD containing all the elements from both RDDs.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.union(rdd2)\n",
    "```\n",
    "\n",
    "### Intersection\n",
    "\n",
    "The `intersection` transformation computes the intersection of two RDDs, i.e., the elements that are present in both RDDs.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.intersection(rdd2)\n",
    "```\n",
    "\n",
    "### Subtract\n",
    "\n",
    "The `subtract` transformation computes the set difference of two RDDs, i.e., the elements present in the first RDD but not in the second RDD.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.subtract(rdd2)\n",
    "```\n",
    "\n",
    "### Cartesian\n",
    "\n",
    "The `cartesian` transformation computes the Cartesian product of two RDDs, i.e., all possible pairs of elements where one element comes from the first RDD and the other from the second RDD.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[(Int, Int)] = rdd1.cartesian(rdd2)\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "val rdd2: RDD[Int] = sc.parallelize(Seq(4, 5, 6, 7, 8))\n",
    "\n",
    "val unionRDD: RDD[Int] = rdd1.union(rdd2)\n",
    "val intersectionRDD: RDD[Int] = rdd1.intersection(rdd2)\n",
    "val subtractRDD: RDD[Int] = rdd1.subtract(rdd2)\n",
    "val cartesianRDD: RDD[(Int, Int)] = rdd1.cartesian(rdd2)\n",
    "\n",
    "println(\"Union: \" + unionRDD.collect().mkString(\", \"))\n",
    "println(\"Intersection: \" + intersectionRDD.collect().mkString(\", \"))\n",
    "println(\"Subtract: \" + subtractRDD.collect().mkString(\", \"))\n",
    "println(\"Cartesian: \" + cartesianRDD.collect().mkString(\", \"))\n",
    "\n",
    "// `rdd1` contains elements 1, 2, 3, 4, 5, and `rdd2` contains elements 4, 5, 6, 7, 8. The `union` operation combines both RDDs, `intersection` computes the common elements, `subtract` computes the elements in `rdd1` but not in `rdd2`, and `cartesian` computes all possible pairs of elements.\n",
    "```\n",
    "\n",
    "### Transformers and accessors\n",
    "In Apache Spark, transformers and accessors are not explicitly defined terms. However, based on the context, it seems like you might be referring to concepts related to data transformation and access in Spark. Here's an explanation of how these concepts might relate to Spark:\n",
    "\n",
    "### Transformers\n",
    "\n",
    "In Spark, transformers are functions or operations that transform an input dataset into a new dataset. These transformations are lazy, meaning they are not executed immediately but are recorded as a lineage of transformations to be applied later when an action is called. Examples of transformers include `map`, `filter`, `groupBy`, `join`, etc.\n",
    "\n",
    "### Accessors\n",
    "\n",
    "Accessors, in the context of Spark, could refer to actions that allow you to access the data in an RDD or DataFrame. Actions are operations that trigger the execution of the lazy transformations and return a result to the driver program or write it to storage. Examples of accessors/actions include `collect`, `count`, `take`, `foreach`, etc.\n",
    "\n",
    "### Example\n",
    "example demonstrating transformers and accessors in Spark:\n",
    "\n",
    "```scala\n",
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "\n",
    "// Transformer: map\n",
    "val squaredRDD = rdd.map(x => x * x)\n",
    "\n",
    "// Accessor: collect\n",
    "val squaredArray = squaredRDD.collect()\n",
    "\n",
    "squaredArray.foreach(println)\n",
    "//`map` is a transformer that squares each element in the RDD `rdd`, creating a new RDD `squaredRDD`. The `collect` method is an accessor that triggers the execution of the transformations and returns the result as an array, which is then printed using `foreach`.\n",
    "```\n",
    "\n",
    "\n",
    "### Lazy Evaluation\n",
    "\n",
    "In Spark, transformations are lazily evaluated, meaning Spark delays executing the transformations until it sees an action that requires a result to be returned to the driver program or saved to storage. This allows Spark to optimize the execution plan by chaining together transformations and minimizing data movement.\n",
    "\n",
    "Example of lazy evaluation:\n",
    "\n",
    "```scala\n",
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "\n",
    "// This is a transformation (lazy)\n",
    "val squaredRDD = rdd.map(x => x * x)\n",
    "\n",
    "// No computation has happened yet\n",
    "\n",
    "// This is an action (eager)\n",
    "val result = squaredRDD.collect()\n",
    "\n",
    "// The transformations are executed here\n",
    "```\n",
    "\n",
    "### Eager Evaluation\n",
    "\n",
    "Actions, on the other hand, trigger the actual computation on the RDDs and return the result. They are eager in nature, meaning they force the execution of the previously defined lazy transformations.\n",
    "\n",
    "Example of eager evaluation:\n",
    "\n",
    "```scala\n",
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "\n",
    "// This is a transformation (lazy)\n",
    "val squaredRDD = rdd.map(x => x * x)\n",
    "\n",
    "// This is an action (eager)\n",
    "val result = squaredRDD.collect()\n",
    "\n",
    "// The transformations are executed here\n",
    "```\n",
    "\n",
    "### Benefits of Lazy Evaluation\n",
    "\n",
    "- **Optimization**: Spark can optimize the execution plan by combining multiple transformations and executing them together.\n",
    "- **Efficiency**: It reduces unnecessary computation by only executing transformations that are required to produce the final result.\n",
    "- **Flexibility**: Allows for more flexible and declarative code, as transformations can be defined without worrying about their order of execution.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
