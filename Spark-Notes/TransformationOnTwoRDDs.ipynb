{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformation on Two RDDs**\n",
    "Apache Spark allows you to perform various transformations on two RDDs, such as union, intersection, subtract, and cartesian. These transformations enable you to combine or manipulate two RDDs in different ways. Here's a detailed explanation of each:\n",
    "\n",
    "### Union\n",
    "\n",
    "The `union` transformation combines two RDDs into a single RDD containing all the elements from both RDDs.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.union(rdd2)\n",
    "```\n",
    "\n",
    "### Intersection\n",
    "\n",
    "The `intersection` transformation computes the intersection of two RDDs, i.e., the elements that are present in both RDDs.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.intersection(rdd2)\n",
    "```\n",
    "\n",
    "### Subtract\n",
    "\n",
    "The `subtract` transformation computes the set difference of two RDDs, i.e., the elements present in the first RDD but not in the second RDD.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[Int] = rdd1.subtract(rdd2)\n",
    "```\n",
    "\n",
    "### Cartesian\n",
    "\n",
    "The `cartesian` transformation computes the Cartesian product of two RDDs, i.e., all possible pairs of elements where one element comes from the first RDD and the other from the second RDD.\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = ...\n",
    "val rdd2: RDD[Int] = ...\n",
    "val result: RDD[(Int, Int)] = rdd1.cartesian(rdd2)\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```scala\n",
    "val rdd1: RDD[Int] = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "val rdd2: RDD[Int] = sc.parallelize(Seq(4, 5, 6, 7, 8))\n",
    "\n",
    "val unionRDD: RDD[Int] = rdd1.union(rdd2)\n",
    "val intersectionRDD: RDD[Int] = rdd1.intersection(rdd2)\n",
    "val subtractRDD: RDD[Int] = rdd1.subtract(rdd2)\n",
    "val cartesianRDD: RDD[(Int, Int)] = rdd1.cartesian(rdd2)\n",
    "\n",
    "println(\"Union: \" + unionRDD.collect().mkString(\", \"))\n",
    "println(\"Intersection: \" + intersectionRDD.collect().mkString(\", \"))\n",
    "println(\"Subtract: \" + subtractRDD.collect().mkString(\", \"))\n",
    "println(\"Cartesian: \" + cartesianRDD.collect().mkString(\", \"))\n",
    "\n",
    "// `rdd1` contains elements 1, 2, 3, 4, 5, and `rdd2` contains elements 4, 5, 6, 7, 8. The `union` operation combines both RDDs, `intersection` computes the common elements, `subtract` computes the elements in `rdd1` but not in `rdd2`, and `cartesian` computes all possible pairs of elements.\n",
    "```\n",
    "\n",
    "### **Transformers and accessors**\n",
    "In Apache Spark, transformers and accessors are not explicitly defined terms. However, based on the context, it seems like you might be referring to concepts related to data transformation and access in Spark. Here's an explanation of how these concepts might relate to Spark:\n",
    "\n",
    "### Transformers\n",
    "\n",
    "In Spark, transformers are functions or operations that transform an input dataset into a new dataset. These transformations are lazy, meaning they are not executed immediately but are recorded as a lineage of transformations to be applied later when an action is called. Examples of transformers include `map`, `filter`, `groupBy`, `join`, etc.\n",
    "\n",
    "### Accessors\n",
    "\n",
    "Accessors, in the context of Spark, could refer to actions that allow you to access the data in an RDD or DataFrame. Actions are operations that trigger the execution of the lazy transformations and return a result to the driver program or write it to storage. Examples of accessors/actions include `collect`, `count`, `take`, `foreach`, etc.\n",
    "\n",
    "### Example\n",
    "example demonstrating transformers and accessors in Spark:\n",
    "\n",
    "```scala\n",
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "\n",
    "// Transformer: map\n",
    "val squaredRDD = rdd.map(x => x * x)\n",
    "\n",
    "// Accessor: collect\n",
    "val squaredArray = squaredRDD.collect()\n",
    "\n",
    "squaredArray.foreach(println)\n",
    "//`map` is a transformer that squares each element in the RDD `rdd`, creating a new RDD `squaredRDD`. The `collect` method is an accessor that triggers the execution of the transformations and returns the result as an array, which is then printed using `foreach`.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
