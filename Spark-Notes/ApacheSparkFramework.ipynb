{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Framework\n",
    "\n",
    "Apache Spark is an open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It is designed to perform both batch processing (similar to MapReduce) and new workloads like interactive queries and stream processing.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Speed**: Spark can perform computations up to 100 times faster than Hadoop MapReduce, primarily due to its in-memory computing capabilities.\n",
    "- **Ease of Use**: Provides easy-to-use APIs for Scala, Java, Python, and R, as well as a rich set of higher-level tools, including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing.\n",
    "- **Generality**: Supports a wide range of applications, from batch data processing to iterative algorithms and interactive queries.\n",
    "- **Fault Tolerance**: Recovers from failures automatically, thanks to its resilient distributed dataset (RDD) abstraction.\n",
    "- **Compatibility**: Runs on Hadoop, Mesos, standalone, or in the cloud and can access diverse data sources, including HDFS, Apache Cassandra, Apache HBase, and S3.\n",
    "\n",
    "### Components\n",
    "\n",
    "1. **Spark Core**: Provides the basic functionality of Spark, including task scheduling, memory management, fault recovery, and interaction with storage systems.\n",
    "   \n",
    "2. **Spark SQL**: Allows SQL queries on data, both inside Spark programs and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC).\n",
    "\n",
    "3. **Spark Streaming**: Enables scalable, fault-tolerant stream processing of live data streams. It ingests data in mini-batches and processes it using RDDs.\n",
    "\n",
    "4. **MLlib (Machine Learning Library)**: Provides a distributed machine learning framework on top of Spark, allowing users to run machine learning algorithms at scale.\n",
    "\n",
    "5. **GraphX**: A graph computation engine built on top of Spark that enables users to interactively build, transform, and reason about graph structured data.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Big Data Processing**: Spark is widely used for processing large-scale datasets, especially in cases where data needs to be processed in near real-time.\n",
    "- **Machine Learning**: MLlib provides scalable machine learning algorithms for tasks like classification, regression, clustering, and collaborative filtering.\n",
    "- **Interactive Analytics**: Spark SQL allows for interactive querying of large datasets using SQL or DataFrame API.\n",
    "- **Streaming Analytics**: Spark Streaming enables the processing of live data streams, such as log files or sensor data, in real-time.\n",
    "\n",
    "### Example\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkExample\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val data = Seq(1, 2, 3, 4, 5)\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val sum = rdd.reduce(_ + _)\n",
    "println(\"Sum: \" + sum)\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "This example creates a Spark session, creates an RDD from a local collection, and computes the sum of the elements in the RDD using the `reduce` action.\n",
    "\n",
    "\n",
    "\n",
    "## RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are the fundamental data structure in Apache Spark, representing a distributed collection of objects that can be operated on in parallel. RDDs are immutable, fault-tolerant, and distributed across nodes in a cluster, making them a key abstraction in Spark for performing parallel processing.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Immutable**: RDDs are immutable, meaning once created, their content cannot be changed. To modify an RDD, you need to create a new RDD derived from the original one.\n",
    "- **Resilient**: RDDs are resilient to failures because Spark automatically reconstructs lost data partitions using lineage information (i.e., the sequence of operations used to build the dataset).\n",
    "- **Distributed**: RDDs are distributed across nodes in a cluster, enabling parallel processing of data.\n",
    "- **Lazy Evaluated**: Transformations on RDDs are lazily evaluated, meaning Spark delays executing the transformation until an action is called. This allows Spark to optimize the execution plan.\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "RDDs can be created from external data sources or by parallelizing an existing collection in the driver program.\n",
    "\n",
    "```scala\n",
    "// Creating an RDD from a local collection\n",
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val rdd = sparkContext.parallelize(data)\n",
    "\n",
    "// Creating an RDD from a text file\n",
    "val textFile = sparkContext.textFile(\"hdfs://path/to/file.txt\")\n",
    "```\n",
    "\n",
    "### Transformations and Actions\n",
    "\n",
    "RDDs support two types of operations: transformations and actions. Transformations create a new RDD from an existing one, while actions compute a result based on an RDD and return it to the driver program or write it to storage.\n",
    "\n",
    "```scala\n",
    "val numbersRDD = sparkContext.parallelize(1 to 10)\n",
    "\n",
    "// Transformation: Filter\n",
    "val evenNumbersRDD = numbersRDD.filter(_ % 2 == 0)\n",
    "\n",
    "// Action: Count\n",
    "val count = evenNumbersRDD.count()\n",
    "```\n",
    "\n",
    "### Example: Word Count\n",
    "\n",
    "Here's an example of using RDDs to perform a word count on a text file:\n",
    "\n",
    "```scala\n",
    "val textFile = sparkContext.textFile(\"hdfs://path/to/file.txt\")\n",
    "val wordCount = textFile\n",
    "  .flatMap(line => line.split(\" \"))\n",
    "  .map(word => (word, 1))\n",
    "  .reduceByKey(_ + _)\n",
    "\n",
    "wordCount.collect().foreach(println)\n",
    "```\n",
    "In this example, `flatMap` is a transformation that splits each line into words, `map` transforms each word into a key-value pair `(word, 1)`, and `reduceByKey` aggregates the counts for each word. The `collect` action collects the result and prints it to the console.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
