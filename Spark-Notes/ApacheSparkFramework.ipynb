{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Framework\n",
    "\n",
    "Apache Spark is an open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It is designed to perform both batch processing (similar to MapReduce) and new workloads like interactive queries and stream processing.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Speed**: Spark can perform computations up to 100 times faster than Hadoop MapReduce, primarily due to its in-memory computing capabilities.\n",
    "- **Ease of Use**: Provides easy-to-use APIs for Scala, Java, Python, and R, as well as a rich set of higher-level tools, including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing.\n",
    "- **Generality**: Supports a wide range of applications, from batch data processing to iterative algorithms and interactive queries.\n",
    "- **Fault Tolerance**: Recovers from failures automatically, thanks to its resilient distributed dataset (RDD) abstraction.\n",
    "- **Compatibility**: Runs on Hadoop, Mesos, standalone, or in the cloud and can access diverse data sources, including HDFS, Apache Cassandra, Apache HBase, and S3.\n",
    "\n",
    "### Components\n",
    "\n",
    "1. **Spark Core**: Provides the basic functionality of Spark, including task scheduling, memory management, fault recovery, and interaction with storage systems.\n",
    "   \n",
    "2. **Spark SQL**: Allows SQL queries on data, both inside Spark programs and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC).\n",
    "\n",
    "3. **Spark Streaming**: Enables scalable, fault-tolerant stream processing of live data streams. It ingests data in mini-batches and processes it using RDDs.\n",
    "\n",
    "4. **MLlib (Machine Learning Library)**: Provides a distributed machine learning framework on top of Spark, allowing users to run machine learning algorithms at scale.\n",
    "\n",
    "5. **GraphX**: A graph computation engine built on top of Spark that enables users to interactively build, transform, and reason about graph structured data.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Big Data Processing**: Spark is widely used for processing large-scale datasets, especially in cases where data needs to be processed in near real-time.\n",
    "- **Machine Learning**: MLlib provides scalable machine learning algorithms for tasks like classification, regression, clustering, and collaborative filtering.\n",
    "- **Interactive Analytics**: Spark SQL allows for interactive querying of large datasets using SQL or DataFrame API.\n",
    "- **Streaming Analytics**: Spark Streaming enables the processing of live data streams, such as log files or sensor data, in real-time.\n",
    "\n",
    "### Example\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkExample\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val data = Seq(1, 2, 3, 4, 5)\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val sum = rdd.reduce(_ + _)\n",
    "println(\"Sum: \" + sum)\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "This example creates a Spark session, creates an RDD from a local collection, and computes the sum of the elements in the RDD using the `reduce` action."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
