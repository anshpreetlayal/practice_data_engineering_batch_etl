{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop and MapReduce\n",
    "\n",
    "Hadoop is an open-source framework for distributed storage and processing of large datasets across clusters of commodity hardware. It is designed to scale from a single server to thousands of machines, each offering local computation and storage. MapReduce is a programming model and processing engine used by Hadoop for processing and generating large data sets.\n",
    "\n",
    "### Components of Hadoop\n",
    "\n",
    "1. **Hadoop Distributed File System (HDFS)**: A distributed file system that provides high-throughput access to application data. It is designed to be fault-tolerant and suitable for use on large clusters of commodity hardware.\n",
    "\n",
    "2. **MapReduce**: A programming model for processing large datasets in parallel across a distributed cluster. It consists of two main phases: the Map phase, where data is processed in parallel, and the Reduce phase, where the results from the Map phase are combined to produce the final output.\n",
    "\n",
    "3. **YARN (Yet Another Resource Negotiator)**: A resource management layer in Hadoop that manages resources in the cluster and schedules jobs.\n",
    "\n",
    "### MapReduce Programming Model\n",
    "\n",
    "- **Map Phase**: In this phase, input data is divided into splits, and a map function is applied to each split in parallel. The output of the map function is a set of key-value pairs.\n",
    "\n",
    "- **Shuffle and Sort Phase**: The output of the map phase is shuffled and sorted by key to group together values associated with the same key.\n",
    "\n",
    "- **Reduce Phase**: In this phase, the reduce function is applied to each group of values with the same key, producing the final output.\n",
    "\n",
    "### Example:\n",
    "Consider a simple word count example using MapReduce:\n",
    "\n",
    "1. **Map Phase**: For each word in the input text, emit a key-value pair `(word, 1)`.\n",
    "\n",
    "2. **Shuffle and Sort Phase**: Group together all `(word, 1)` pairs with the same word.\n",
    "\n",
    "3. **Reduce Phase**: For each group of `(word, [1, 1, 1, ...])` pairs, sum up the counts to get the total count for that word.\n",
    "\n",
    "### Hadoop Ecosystem\n",
    "\n",
    "Hadoop has a rich ecosystem of tools and libraries that extend its functionality:\n",
    "\n",
    "- **Apache Hive**: A data warehouse infrastructure built on top of Hadoop that provides a SQL-like query language (HiveQL) for querying and managing large datasets.\n",
    "\n",
    "- **Apache Pig**: A high-level platform for creating MapReduce programs used for analyzing large datasets.\n",
    "\n",
    "- **Apache Spark**: An open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It can run on top of Hadoop YARN.\n",
    "\n",
    "- **Apache HBase**: A distributed, scalable, big data store that provides random, real-time read/write access to large datasets.\n",
    "\n",
    "## Differences between Spark and Hadoop\n",
    "Apache Spark and Hadoop are both frameworks used for big data processing, but they have some key differences in terms of architecture, performance, and ease of use. Here are some of the main differences between Spark and Hadoop:\n",
    "\n",
    "1. **Processing Model**:\n",
    "   - **Hadoop**: Hadoop uses the MapReduce programming model, which involves reading data from disk, processing it, and writing the results back to disk between each Map and Reduce phase. This can lead to high disk I/O overhead.\n",
    "   - **Spark**: Spark, on the other hand, uses an in-memory processing model, where intermediate data is kept in memory rather than written to disk after each stage of processing. This results in significantly faster processing times compared to Hadoop's MapReduce.\n",
    "\n",
    "2. **Performance**:\n",
    "   - **Hadoop**: Hadoop's performance is limited by its disk-based processing model, which can lead to high latency for iterative algorithms or interactive queries.\n",
    "   - **Spark**: Spark's in-memory processing model allows it to perform much faster than Hadoop, especially for iterative algorithms (e.g., machine learning) or interactive queries.\n",
    "\n",
    "3. **Ease of Use**:\n",
    "   - **Hadoop**: Hadoop's programming model (MapReduce) requires developers to write low-level, imperative code, which can be complex and error-prone.\n",
    "   - **Spark**: Spark provides higher-level APIs (e.g., RDDs, DataFrames, and Datasets) that make it easier to write parallel applications. It also supports multiple languages (Scala, Java, Python, and R), making it more accessible to a wider range of developers.\n",
    "\n",
    "4. **Fault Tolerance**:\n",
    "   - **Hadoop**: Hadoop provides fault tolerance by replicating data across multiple nodes in the cluster. If a node fails, the data can be recovered from its replicas.\n",
    "   - **Spark**: Spark also provides fault tolerance but uses a different approach called lineage. Spark keeps track of the sequence of transformations applied to a dataset (RDD), so if a partition is lost, it can be recomputed based on the transformations.\n",
    "\n",
    "5. **Use Cases**:\n",
    "   - **Hadoop**: Hadoop is well-suited for batch processing of large datasets, particularly when fault tolerance and scalability are important.\n",
    "   - **Spark**: Spark is more versatile and can be used for batch processing, interactive queries, stream processing, and machine learning. It is particularly well-suited for iterative algorithms and real-time processing.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
